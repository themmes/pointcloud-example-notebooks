{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook for processing point cloud data for PointNet\n",
    "\n",
    "For this example I simply downloaded the \"Oakland\" dataset (training) http://www.cs.cmu.edu/~vmr/datasets/oakland_3d/cvpr09/doc/ and converted the dataset to multiple LAZ files for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from laspy.file import File\n",
    "import morton\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "# also requires fastparquet\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading text based point cloud (for LAS/LAZ see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/home/tom/vision/data/training')\n",
    "data = data_dir.joinpath('oakland_part3_an_training.xyz_label_conf')\n",
    "df = dd.read_csv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading LAS (or LAZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/tom/vision/data/training/oakland_part3_an_training.las')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasfile_dir = Path('/home/tom/vision/data/training')\n",
    "lasfiles = sorted(list(lasfile_dir.glob('*.las')))\n",
    "lasfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = ['flag_byte', 'scan_angle_rank', 'user_data', 'pt_src_id']\n",
    "meta = pd.DataFrame(np.empty(0, dtype=[('X',float),('Y',float),('Z',float),\n",
    "                                       ('intensity',float),('raw_classification',int)]))\n",
    "\n",
    "@delayed\n",
    "def load(file):\n",
    "    with File(file.as_posix(), mode='r') as las_data:\n",
    "        las_df = pd.DataFrame(las_data.points['point'], dtype=float).drop(dropped_columns, axis=1)\n",
    "        return las_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [load(file) for file in lasfiles]\n",
    "df = dd.from_delayed(dfs, meta=meta)\n",
    "df = df.repartition(npartitions=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I often write intermediate steps to Parquet storage to be able to experiment freely with the dataframe. I believe loading Parquet is not (or not much) faster than loading LAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('/home/tom/vision/data/training/oakland', compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/home/tom/vision/data/training/oakland')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['X'] = df.X - df.X.min()\n",
    "df['Y'] = df.Y = df.Y.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 72 ms, total: 2.02 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%time df.to_parquet('/home/tom/vision/data/training/oakland_trans', compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute grid cell identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/home/tom/vision/data/training/oakland_trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5.0 #meters\n",
    "m = morton.Morton(dimensions=2, bits=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(point, grid_size=grid_size):\n",
    "    return m.pack(int(point.X // grid_size), int(point.Y // grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hash'] = df[['X', 'Y']].apply(get_hash, grid_size=grid_size, meta=('hash', int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_parquet('/home/tom/vision/data/training/oakland_hash', compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/home/tom/vision/data/training/oakland_hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(np.empty(0, dtype=list(zip(list(df.columns), list(df.dtypes))) + \\\n",
    "                             list(zip(['XN', 'YN'], [np.dtype('float64')]*2))))\n",
    "\n",
    "def normalise(df):\n",
    "    df = df.copy()\n",
    "    df['XN'] = (df.X - df.X.mean()) / (df.X.max() - df.X.min())\n",
    "    df['YN'] = (df.Y - df.Y.mean()) / (df.Y.max() - df.Y.min())\n",
    "    return df\n",
    "\n",
    "df = df.groupby('hash').apply(normalise, meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ZN'] = (df.Z - df.Z.mean()) / (df.Z.max() - df.Z.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.to_parquet('/home/tom/vision/data/training/oakland_norm', compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for data preparation, the final normalized dataset is your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset\n",
    "\n",
    "Before training and testing the model you should split this dataset into `train`, `test` and `validation`. I also implemented this code in the `train_custom.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet('/home/tom/vision/data/training/oakland_norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = df.index.unique().compute().values\n",
    "\n",
    "train_test_msk = np.random.rand(len(hashes))\n",
    "train_val_hashes = hashes[train_test_msk < 0.8]\n",
    "test_hashes = hashes[~(train_test_msk < 0.8)]\n",
    "\n",
    "train_val_msk = np.random.rand(len(train_val_hashes))\n",
    "train_hashes = train_val_hashes[train_val_msk < 0.8]\n",
    "validation_hashes = train_val_hashes[~(train_val_msk < 0.8)]\n",
    "\n",
    "with open('/home/tom/vision/data/training/data_split.json', 'w') as data_split:\n",
    "    json.dump({'train': train_hashes.tolist(), 'validation': validation_hashes.tolist(), 'test': test_hashes.tolist()}, data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "To feed the data to the deep learning network you need a generator. I also implemented this code in the `train_custom.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def generator(df, hashes, BATCH_SIZE, NUM_POINT, N_AUGMENTATIONS, shuffled=True):\n",
    "    \"\"\"\n",
    "    Generator function to serve the data to the algorithm.\n",
    "    \n",
    "    IN: df (the entire dataframe), hashes (the indices to serve), \n",
    "        BATCH_SIZE and NUM_POINTS (to set output shape),\n",
    "        N_AUGMENTATIONS (the number of \"augmentations\" or iterations of sampling)\n",
    "    OUT: data, label (batch of data and corresponding labels)\n",
    "    \"\"\"\n",
    "    data_channels = ['X', 'Y', 'Z', 'XN', 'YN','intensity']\n",
    "    \n",
    "    seed_hash = []\n",
    "    for seed in range(N_AUGMENTATIONS):\n",
    "        for h in hashes:\n",
    "            seed_hash.append((seed, h))\n",
    "    shuffle(seed_hash)\n",
    "    \n",
    "    batches = [seed_hash[i:i+BATCH_SIZE] for i in range(0,len(seed_hash),BATCH_SIZE)]\n",
    "    if len(batches[-1]) < BATCH_SIZE: batches = batches[:-1]\n",
    "    if shuffled: [shuffle(batch) for batch in batches]\n",
    "        \n",
    "    def random_sample_block(group, seed):\n",
    "        \"\"\"\n",
    "        Sample entirely random for the entire grid cell\n",
    "        IN: group (all points in a grid cell), seed (random state value)\n",
    "        OUT: data_group (a subset of the points in the grid cell; a training sample)\n",
    "        \"\"\"\n",
    "        if len(group) > NUM_POINT:\n",
    "            data_group = group.sample(n=NUM_POINT, replace=False, random_state=seed)\n",
    "        else:\n",
    "            data_group = group.sample(n=NUM_POINT, replace=True, random_state=seed)\n",
    "        return data_group\n",
    "\n",
    "    for batch in batches:\n",
    "        df_batch = [random_sample_block(df.loc[h], s) for s,h in batch]\n",
    "        data = np.stack([b[data_channels].values for b in df_batch])\n",
    "        label = np.stack([l.label.values for l in df_batch])\n",
    "        yield data, label\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting `train.py`\n",
    "\n",
    "Check out the `train_custom.py` for my adaptations to the `train.py` from the original PointNet codebase. This implements the data splitting and generator I mentioned earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tom/vision/pointnet/sem_seg/') \n",
    "print(os.getcwd())\n",
    "%run train_custom.py --log_dir=log --max_epoch=50 --num_point=4000 --batch_size=12 --n_augmentations=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
